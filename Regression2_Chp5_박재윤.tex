\documentclass{beamer}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
	\usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
	\usecolortheme{default} % or try albatross, beaver, crane, ...
	\usefonttheme{default}  % or try serif, structurebold, ...
	\setbeamertemplate{navigation symbols}{}
	\setbeamertemplate{caption}[numbered]
	\setbeamertemplate{footline}[frame number]
} 

% \usepackage{kotex} 

\usepackage{times}
\usepackage{amsmath}
\usepackage{multirow}

\newenvironment{rcases}
{\left.\begin{aligned}}
	{\end{aligned}\right\rbrace}


\usepackage{color}
\pagecolor{yellow}
\title{ {\Large \bf Regression Analysis II }  }  
\subtitle{Chapter 5. Selection of Regression Models}
\author{Hojin Yang}
\institute{Department of Statistics\\ Pusan National University}
\date{}



\begin{document}
	\scriptsize
	
	
	\begin{frame}
		\titlepage
	\end{frame}
	
	% Uncomment these lines for an automatically generated outline.
	%\begin{frame}{Outline}
	%  \ofcontents
	%\end{frame}
	
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
	
	\frame{ 
		\frametitle{5.1 Model Selection}
		\textbf{(1) Model Selection} \\
		\vspace{1em}
		- Two goals of regression analysis \\
		\vspace{0.5em}
		\quad (i) fit the relation between the response and the covariates \\
		\vspace{0.5em}
		\quad (ii) prediction based on the fitted model \\
		\vspace{1em}  
		- Model selection criterion : prediction error \\
		\vspace{2em}
		
		\textbf{(2) Prediction error} \\
		\vspace{1em}
		- Data \\
		\vspace{0.5em}
		\quad (i) \emph{training data} : data used in model fitting \\
		\vspace{0.5em}
		\quad (ii) \emph{test data} : data used in assessing the goodness-of-fit of the fitted model \\
		\vspace{ 0.5em}  
		- Prediction error \\
		\vspace{0.5em}
		\quad Let \textbf{X} = ($X_1, ... , X_p$) be covariates, $Y$ be the response, and $f$ be the regression \\
		\vspace{0.5em}
		\quad function to be estimated. If $L$ is a loss function, then
		$$
		f = \underset{g}{\arg \min} E[L(Y, g(\textbf{X}))]
		$$
		
	}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.1 Model Selection}
		If a squared error loss $L(Y, g(\textbf{X})) = (Y - g(\textbf{X}))^2$ is given, then
		$$
		f(\textbf{x}) = E(Y|\textbf{X} = \textbf{x})
		$$
		Let $\hat{f}$ be the estimate of $f$ based on the training data $\chi_n = {(\textbf{X}_i, Y_i), i=1, ... , n}$ Then, the prediction error based on the training data \textbf{T} = (\textbf{Z},W) is given by
		$$
		PE = E_{\chi_n}E_{\textbf{Z},W}L(W, \hat{f}(\textbf{Z}))
		$$
		\vspace{ 0.5em}  
		Now, we apply the prediction error to the linear regression model. Assume that the model is 
		$$
		Y = \mu + \varepsilon,
		$$
		$$
		where \; E(\varepsilon) = 0, Var(\varepsilon) = \sigma^2.
		$$
		
	}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	 
	\frame{
		\frametitle{5.1 Model Selection}
		
		Let $\hat{f(\textbf{x})}$ = \textbf{x}$'$$\hat{\boldsymbol{\beta}}$, where $\hat{\boldsymbol{\beta}}$ = $(\textbf{X}'\textbf{X})^{-1}\textbf{X}'\textbf{y}$ is based on the training data \\
		\vspace{0.8em}
		$\chi_n$ = $\left\{(\textbf{X}_i, Y_i), i=1, ..., n\right\}$.
		Now, for the test data \textbf{T} = (\textbf{Z},W), the prediction error can be \\
		\vspace{0.8em}
		written as
		$$ \begin{aligned}
			PE & =E_{\textbf{Z},W}E_{\chi_n} (W-\textbf{z}'\hat{\boldsymbol{\beta}})^2 \\
			& = E_{\textbf{Z},W}E_{\chi_n} (W-\mu + \mu - \textbf{z}'\hat{\boldsymbol{\beta}})^2  \\
			& = E_{\textbf{Z},W}E_{\chi_n} {(W-\mu)^2 + (\mu-\textbf{z}'\hat{\boldsymbol{\beta}})^2 + 2(W-\mu)(\mu-\textbf{z}'\hat{\boldsymbol{\beta}})} \\
			& = \sigma^2 + E_{\textbf{Z},W}E_{\chi_n} (\mu - \textbf{z}'\hat{\boldsymbol{\beta}})
		\end{aligned}
		$$
		where $\sigma^2$ is called \emph{irreducible error}, and the 2nd term is called \emph{ME(model error)}.
		
	}
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.1 Model Selection}
		
		Therefore, minimizing the prediction error is equivalent to minimizing the model error. \\
		\vspace{0.5em}
		Now, we let (\textbf{X, $\omega$}) be the test data, then
		$$ \begin{aligned}
			ME & = E[(\boldsymbol{\mu}-\textbf{X}'\hat{\boldsymbol{\beta}})'(\boldsymbol{\mu}-\textbf{X}'\hat{\boldsymbol{\beta}})] \\
			& = E[(\boldsymbol{\mu}-\textbf{Hy})'(\boldsymbol{\mu}-\textbf{Hy})] \\
			& = E[(\boldsymbol{\mu}-\textbf{H}\boldsymbol{\mu}-\textbf{H}\varepsilon)'(\boldsymbol{\mu}-\textbf{H}\boldsymbol{\mu}-\textbf{H}\boldsymbol{\varepsilon})] \\
			& = E[\left\{(\textbf{I}-\textbf{H})\boldsymbol{\mu}-\textbf{H}\boldsymbol{\varepsilon}\right\}'\left\{(\textbf{I}-\textbf{H})\boldsymbol{\mu}-\textbf{H}\boldsymbol{\varepsilon}\right\}] \\
			& = E[\boldsymbol{\mu}'(\textbf{I}-\textbf{H})\boldsymbol{\mu} + \boldsymbol{\varepsilon}'\textbf{H}\boldsymbol{\varepsilon}] \\
			& = \boldsymbol{\mu}'(\textbf{I}-\textbf{H})\boldsymbol{\mu} + \rho\sigma^2
		\end{aligned} $$
		where, $H=\textbf{X}(\textbf{X}'\textbf{X})^{-1}\textbf{X}'.$ Note that $Bias(\textbf{X}'\hat{\boldsymbol{\beta}})$ = $E(\textbf{X}'\hat{\boldsymbol{\beta}})$ $-$ $\boldsymbol{\mu}$, so that 
		$$ \begin{aligned}
			\boldsymbol{\mu}'(\textbf{I}-\textbf{H})\boldsymbol{\mu} & = Bias(\textbf{X}'\hat{\boldsymbol{\beta}})'Bias(\textbf{X}'\hat{\boldsymbol{\beta}})
		\end{aligned} $$
		and
		$$
		\rho\sigma^2 = tr[Cov(\textbf{X}'\hat{\boldsymbol{\beta}})]
		$$
		Therefore, the model error is the same as the MSE (mean squared error)
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.1 Model Selection}
		\textbf{Example for Model Selection} \\
		\vspace{1em}
		Let $\mu$ $\equiv$ $\mu(x)$ = $exp(0.1 + 0.02x^2)$ be a true model, and we generate 100 random \\
		\vspace{0.5em}
		numbers $(x_i, y_i), i=1, \cdots, 100, $ where $x = 1(0.09)(10)$ and $\varepsilon_i \sim N(0, {1.5}^2)$, i.e.,
		$$
		Y_i = exp(0.1 + 0.02x_i^2) + \varepsilon_i
		$$
		and our postulated model is
		$$
		f_6(x) = \sum_{j=0}^{6} \beta_jx^j
		$$
		\\
		\vspace{1em}
		See Fig. 5.1(p.207) ; \\
		\vspace{0.5em}
		$\mu(5)$ = $exp(0.1 + 0.02\times5^2)$ = 1.822 vs $\hat{f_j}(5)$
		
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.2 Criteria for Variable Selection}
		
		Assume that the number of all possible candidate covariates is (k-1) \\
		\vspace{1em}
		- Types of models  \\
		\vspace{0.5em}
		$(i)$ $maximal (full, saturated)$ model : use $k$-1 variables in fit \\
		\vspace{0.5em}
		$(ii)$ $current$ model : use p-1 variables (p $<$ k) \\ 
		\vspace{0.5em}
		$(iii)$ $minimal(null)$ model : do not use any covariates \\
		\vspace{1em}
		Ex. 5.1(p.208) : house price data($X_1, X_2, X_3, X_4$) \\
		\vspace{0.5em}
		- Fit using the full model
		$$
		\hat{Y} = 1.22 + 0.0519X_1 + 0.0116X_2 + 0.349X_3 - 0.219X_4
		$$
		with $R^2$ = 0.93 \\
		
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.2 Criteria for Variable Selection}
		\textbf{5.2.1 $\boldsymbol{R_p^2}$} \\
		$$
		R_p^2 = \frac{SSR_p}{SST} = 1-\frac{SSE_p}{SST}
		$$
		\\
		\vspace{0.5em}
		See Fig. 5.3(p.211) \\
		\vspace{3em}
		\textbf{5.2.2 $\boldsymbol{R^2_{ap}}$ and $\boldsymbol{s^2_p}$}
		$$
		R^2_{ap} = 1-\frac{SSE_p / (n-p)}{SST / (n-1)} = 1 - \frac{s^2_p}{SST / (n-1)}
		$$
		See Fig. 5.4 (p.212)  \\
		\vspace{2em}

		
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.2 Criteria for Variable Selection}
		\textbf{5.2.3 Mallows' $\boldsymbol{C_p}$} \\
		\vspace{1em}
		(1) Motivation \\
		Minimize total mean squared errors \\
		\vspace{1em}
		(2) Derivation of $C_p$ \\
		Assume that the true regression model is
		$$
		\textbf{y} = \boldsymbol{\mu} + \boldsymbol{\varepsilon},
		$$
		where $E(\textbf{y}) = \boldsymbol{\mu} = (\mu_1, ..., \mu_n)^T$.
		 On the other hand, the postulated model is $\textbf{y} = \textbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}$. Let $\hat{\textbf{y}}$ be the fitted vector under the postulated model, then the mean squared error of the $i$-th observation is 
		$$
		E(\hat{y}_i - \mu_i)^2
		$$
		which can be rewritten as
		$$ \begin{aligned}
			E(\hat{y}_i - \mu_i)^2 & = E\left\{(\hat{y}_i - E(\hat{y}_i)) + (E(\hat{y}_i) - \mu_i)\right\}^2 \\
			& = Var(\hat{y}_i) + \left\{E(\hat{y}_i) - \mu_i\right\}^2,
		\end{aligned} 
		$$
		i.e., $MSE(\hat{\theta}) + Bias^2(\hat{\theta})$.
		
		
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.2 Criteria for Variable Selection}
		Now, we want to minimize the total MSE,
		$$ \begin{aligned}
			\Gamma_p & = \sum_{i=1}^{n} \left\{Var(\hat{y}_i) + (E(\hat{y}_i) - \mu_i)^2\right\} / \sigma^2 \\
			& = \frac{1}{\sigma^2} \left\{tr[Cov(\hat{\textbf{y}})] + (E(\hat{\textbf{y}}) - \boldsymbol{\mu})'(E(\hat{\textbf{y}}) - \boldsymbol{\mu})\right\}.
		\end{aligned}
		$$
		Note that
		\vspace{0.5em}
		$$
		tr[Cov(\hat{\textbf{y}})] = tr[Cov(\textbf{Hy})] = tr[\textbf{X}(\textbf{X}'\textbf{X})^{-1}\textbf{X}'\cdot\sigma^2] = \rho\sigma^2
		$$
		\vspace{0.5em}
		and \\
		$$
		E(\hat{\textbf{y}}) - \boldsymbol{\mu} = E(\textbf{Hy}) - \boldsymbol{\mu} = -(\textbf{I}-\textbf{H})\boldsymbol{\mu}
		$$
		Hence, we have
		$$
		\Gamma_p = p + \frac{1}{\sigma^2}\boldsymbol{\mu}'(\textbf{I}-\textbf{H})\boldsymbol{\mu}
		$$
		which contains unknown parameters such as $\sigma^2$, $\boldsymbol{\mu}$, we need to estimate them. \\
		\vspace{0.5em}
		
	}		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.2 Criteria for Variable Selection}
		Note that
		$$
		E(SSE_p) = E(\textbf{e}'\textbf{e}) = E[\textbf{y}'(\textbf{I}-\textbf{H})\textbf{y}] = \boldsymbol{\mu}'(\textbf{I}-\textbf{H})\boldsymbol{\mu} + (n-p)\sigma^2,
		$$
		where $SSE_p$ is the residual sum of squares under the current model with p-1 \\
		\vspace{0.5em}
		covariates. Therefore,
		$$
		\frac{E(SSE_p)}{\sigma^2} - (n-2p) = \Gamma_p
		$$
		Hence, as an estimator for $\Gamma_p$, Mallows(1973) suggested to use
		$$
		C_p = \frac{SSE_p}{s^2} - (n-2p),
		$$
		where $s^2$ is an estimator of $\sigma^2$ under the maximal model.
	}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\frame{
		(3) Properties of $C_p$ \\
		\vspace{1em}
		$(i)$ $C_p$ = Goodness-of-fit + Penalty term \\
		\vspace{0.5em}
		\quad \; 1st term = $SSE_p / s^2$ : decreases as the number of covariates increases \\
		\vspace{0.5em}
		\quad \; 2nd term = 2$p - n$ : increases as the number of covarates increases \\
		\vspace{1em}
		Hence, $C_p$ is often called \emph{penalized criterion}. \\
		\vspace{2em}
		(ii) $C_p$ $\simeq$ $p$ \\
		\vspace{0.5em}
		If the current model is good, then $SSE_p$ $\simeq$ $(n-p)s^2$, and therefore, \\
		\vspace{0.5em}
		$C_p$ $\simeq$ $(n-p) - (n-2p) = p.$ \\
		\vspace{1em}
		  In fact, under the maximal model, we have $SSE_p$ = $(n-p)s^2$, so that $C_p$ = $p$ \\
		\vspace{2em}
		See Fig. 5.5 (p.215)
		
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.2 Criteria for Variable Selection}
		\textbf{5.2.4 $\boldsymbol{PRESS_p}$} \\
		\vspace{1em}
		(1) Motivation \\
		\vspace{0.5em}
		Minimize the PRESS (PREdiction Sum of Squared errors) suggested by Allen (1971). Use $n$ - 1 observations as training set, and use 1 observation as prediction (test set). \\
		\vspace{1em}
		(2) Derivation 
		$$
		PRESS_p = \sum_{i=1}^{n} (y_i - \hat{y}_{i(i)})^2,
		$$
		where $\hat{y}_{i(i)}$ is the fitted value at $x_i$ based on $n - 1$ observations. Note that
		$$
		\hat{y}_i(i) = \textbf{x}_i'\hat{\boldsymbol{\beta}}_{(i)} = \textbf{x}_i'\left(\hat{\boldsymbol{\beta}} - \frac{(\textbf{X}'\textbf{X})^{-1}\textbf{x}_ie_i}{1 - h_{ii}}\right) = \hat{y}_i - \frac{h_{ii}e_i}{1 - h_{ii}}
		$$
		and therefore,
		$$
		PRESS_p = \sum_{i=1}^{n} \left(y_i - \hat{y}_i + \frac{h_{ii}e_i}{1 - h_{ii}}\right)^2 = \sum_{i=1}^{n} \left(\frac{e_i}{1 - h_{ii}}\right)^2
		$$
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.3 Methods of Variable Selection}
		$(i)$ All possible regression \\
		\vspace{0.5em}
		$(ii)$ Forward stepwise regression \\
		\vspace{0.5em}
		$(iii)$ Forward selection \\
		\vspace{0.5em}
		$(iv)$ Backward elimination \\
		\vspace{4em}
		\textbf{5.3.1 All Possible Regression} \\
		\vspace{0.5em}
		Assume that the maximal model has $k-1$ covariates. Then the number of \\
		\vspace{0.5em}
		possible models are \\
		$$
		(\frac{k-1}{0}) + (\frac{k-1}{1}) + \cdots + (\frac{k-1}{k-1}) = 2^{k-1}
		$$
		\\
		\vspace{0.5em}
		which is almost computationally infeasible if $k$ is large.
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.3 Methods of Variable Selection}
		\textbf{5.3.2 Forward Stepwise Regression} \\
		\vspace{1em}
		\textbf{STEP1}. Assume that the current model has 1 covariate, and make an $F$-test
		$$
		F^*_i = \frac{MSR_i}{s^2_i}, i=1, \cdots, k-1
		$$
		and if $\max_{1 \leq i \leq k-1} F^*_i > F_\alpha (1, n-2)$, then the corresponding variable is entered. \\
		\vspace{0.5em}
		Otherwise, select the null model. \\
		\vspace{1em}
		\textbf{STEP2}. Assume that $X_7$ has been selected in STEP1. Do the partial $F$-test for other \\
		\vspace{0.5em}
		variables than $X_7$, i.e., 
		$$
		F^*_i = \frac{MSR(X_i | X_7)}{s^2(X_7, X_i)} = \left(\frac{\hat{\beta_i}}{SE(\hat{\beta_i})}\right)^2, i \neq 7
		$$
		and if $max_{i \neq 7} F^*_i > F_\alpha (1, n-3)$, then the corresponding variable is entered. \\
		\vspace{0.5em}
		Otherwise, stop.
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.3 Methods of Variable Selection}
		\textbf{STEP3}. Assume that $X_3$ has been selected in STEP2. Do the partial $F$-test for $X_7$ \\
		\vspace{0.5em}
		when $X_3$ is given, i.e., 
		$$
		F^*_7 = \frac{MSR(X_7 | X_3)}{s^2(X_3, X_7)}
		$$
		and if it is larger than $F_\alpha (1, n-3)$, then $X_7$ remains in the model. Otherwise, $X_7$ is \\
		\vspace{0.5em}
		deleted from the model. \\
		\vspace{1em}
		\textbf{STEP4}. If $X_7$ remains the model in STEP3, then repeat STEP2 and STEP3. \\
		\vspace{2em}
		EX. 5.2 (p.219)
		
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.3 Methods of Variable Selection}
		\textbf{5.3.3 Forward selection} \\
		\vspace{1em}
		Start with the null model, and the variable with largest F-value is entered \\
		\vspace{0.5em}
		if it is significant. We continue this process until no further significant variable appears. \\
		\vspace{0.5em}
		Note that if some variable is entered, it never removed from the model. \\
		\vspace{2em}
		\textbf{5.3.4 Backward Elimination} \\
		\vspace{1em}
		Start with the maximal model, and the variable with smallest F-value is
		removed \\
		\vspace{0.5em}
		if it is not significant. We continue this process until no further
		non-significant variable \\
		\vspace{0.5em}
		appears. Note that if some variable is removed, it never remains in the model.
		$$
		F^*_1 = \frac{MSR(X_1 | X_2, \cdots , X_{k-1})}{s^2(X_1, \cdots , X_{k-1})}
		$$
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.4 Model Validation}
		\textbf{5.4.1 Cross validation} \\
		\vspace{1em}
		(1) Motivation \\
		\vspace{1em}
		For the estimation of prediction error, the \emph{Cross-Validation}(CV) is often used. \\
		\vspace{0.5em}
		The idea is partition $n$ observations into two sets; one is the training set and the other \\
		\vspace{0.5em}
		is the test set. \\
		\vspace{1em}
		(2) Definition \\
		\vspace{1em}
		Partition a set $\left\{1, \cdots, n\right\}$ into $K$ sets $I_j, j=1, \cdots, K$, and define a function \\
		\vspace{0.5em}
		$\kappa : \left\{1, \cdots, n\right\} \mapsto \left\{1, \cdots, K\right\}$ such that $\kappa(i) = j \Leftrightarrow i \in I_j$. \\
		\vspace{0.5em}
		For the data set $\left\{(\textbf{X}_1,Y_1), \cdots, (\textbf{X}_n, Y_n)\right\}$, let $\hat{f}_{-j}$ be the estimator based on \\
		\vspace{0.5em}
		observations except in $I_j$. Then, the cross-validation estimation for the prediction error \\
		\vspace{0.5em}
		is defined as
		$$
		\hat{PE}_{CV} = \frac{1}{n}\sum_{i=1}^{n} L(Y_i, \hat{f}_{-\kappa(i)}(\textbf{X}_i)), 
		$$
		where $L$ is a loss function.
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.4 Model Validation}
		(3) k-fold CV \\
		\vspace{0.5em}
		When we partition $n$ observations into $k$ groups with equal size, i.e., $n$ = $kc$ for some $c$, \\
		\vspace{0.5em} 
		then $k$-fold cross validation uses $k-1$ group for the training set and one group for the \\
		\vspace{0.5em}
		test set. As a special case, if $K$ = $n$ and $c$ = 1, then it is called LOOCV \\
		\vspace{0.5em}
		(Leave-One-Out-Cross-Validation), i.e., 1-fold CV. It is recommended that either $k$=5 or \\
		\vspace{0.5em}
		$k$ =10. \\
		\vspace{1em}
		(4) Example \\
		\vspace{0.5em}
		Assume that $n$ = 6, $k$ = 3, and 3 groups are $\left\{1,5\right\}$, $\left\{2,6\right\}$, $\left\{3,4\right\}$
		\begin{table}
			\begin{center}
				\begin{tabular}{c|c}
					\hline
					training data & test data \\
					\hline
					1,2,5,6 & 3,4 \\
					1,3,4,5 & 2,6 \\
					2,3,4,6 & 1,5 \\
					\hline
				\end{tabular}
			\end{center}
		\end{table}
		\vspace{2em}
		\textbf{5.4.2 Training data, Test data, and Prediction error} \\
		\vspace{0.5em}
		See Fig. 5.8 (p.222)
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.5 Diagnostics on the Variable Selection}
		Let $K$ = $\left\{i_1, \cdots, i_k\right\}$ be an index set of size $k$. Here, we want to see the effect of $k$ \\
		\vspace{0.5em}
		observations in $K$ on Mallows' $C_p$. Recall that
		$$
		C_p = \frac{SSE_p}{s^2} - (n-2p)
		$$
		Let $\textbf{e}^c$ and $\textbf{e}$ be the residual vectors under the current model and the maximal model, \\
		\vspace{0.5em}
		respectively. If $q-1$ is the number of covariates under the maximal model, then
		$$
		C_p = \frac{\textbf{e}^{c'}\textbf{e}^c}{\textbf{e}'\textbf{e} / (n-q)} - (n-2p)
		$$
		Now, the $C_p$ based on $n - k$ observations after deleting $k$ observations in $K$ is
		$$
		C_{p(K)} = \frac{SSE_{p(K)}}{s^2_{(K)}} - (n - k -2 p)
		$$
		
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.5 Diagnostics on the Variable Selection}
		and it can be shown that
		$$
		C_{p(K)} = \frac{(n-k-q)\left\{C_p + (n-2p) - \textbf{e}^{c'}_K(\textbf{I}-\textbf{H}^c_K)^{-1}\textbf{e}^c_K / s^2\right\}}{(n - q) - e_K(\textbf{I}-\textbf{H}_K)^{-1}\textbf{e}_K / s^2} - (n - k - 2p)
		$$
		\vspace{2em}
		Ex. 5.3 (p.224)
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.6 Miscellanea in Model Selection}
		\textbf{5.6.1 Jensen inequality and the Kullback-Leibler distance} \\
		\vspace{2em}
		\textbf{Theorem 5.1 (Jensen's inequality)} \\
		\vspace{1em}
		Let $\phi$ be a convex function in an open interval $I$, and assume that $P (X \in I)$ = 1 and \\
		\vspace{0.5em}
		$E(X) < \infty$. Then, we have
		\\
		$$
		\phi[E(X)] \leq E[\phi(X)]
		$$  
		\\
		which is called \emph{Jensen's inequality}. \\
		\vspace{2em}
		\textbf{Definition 5.1 (Kullback-Leibler distance}) \\
		\vspace{1em}
		Assume that two pdfs $f$ and $g$ are given. Then, the Kullback-Leibler distance between \\
		\vspace{0.5em}
		$g$ and $f$, Kullback-Leibler information of $g$ at $f$, or the entropy distance between $f$ and $g$ \\
		\vspace{0.5em}
		with respect to $f$ is
		$$
		E_f\left[\log\left(\frac{f(X)}{g(X)}\right)\right] = \int \log\left(\frac{f(x)}{g(x)}\right)f(x)dx
		$$
		
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.6 Miscellanea in Model Selection}
		As a corollary, we have
		$$ \begin{aligned}
			E_f\left[\log\left(\frac{f(X)}{g(X)}\right)\right]\; & =  -E_f\left[\log\left(\frac{g(X)}{f(X)}\right)\right] \\
			& \leq -\log\left[E_f\left(\frac{g(X)}{f(X)}\right)\right] \\
			& = 0
		\end{aligned}
		$$
		Note that Kullback-Leibler distance is always non-negative, and 0 if $f$ = $g$.
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.6 Miscellanea in Model Selection}
		\textbf{5.6.2 Akaike information criterion} \\
		\vspace{0.5em}
		Let $\boldsymbol{\theta}_0 \in R^p$ be the true model and $\boldsymbol{\theta} \in R^p$ be the postulated model. Also, let \\
		\vspace{0.5em}
		$f(x) \equiv f(x ; \boldsymbol{\theta}_0)$ and $f(x; \boldsymbol{\theta})$ be the pdfs under the true and postulated model,\\
		\vspace{0.5em}
		respectively. Now, let
		$$
		K(\boldsymbol{\theta}) = -2E_{\boldsymbol{\theta}_0} [\log f(X ; \boldsymbol{\theta})] = -2 \int \log f(x ; \boldsymbol{\theta})f(x)dx
		$$
		and consider
		$$
		\frac{1}{2} K(\boldsymbol{\theta}) + \int \left\{\log f(x)\right\}f(x)dx
		$$
		which is Kullback-Leibler distance because if we let $g(x) \equiv f(x ; \boldsymbol{\theta})$, then
		$$ \begin{aligned}
			\frac{1}{2}K(\boldsymbol{\theta}) + \int \left\{\log f(x)\right\}f(x)dx & = - \int \log f(x;\boldsymbol{\theta})f(x)dx + \int\left\{\log f(x)\right\}f(x)dx \\
			& = \int \log\left(\frac{f(x)}{g(x)}\right)f(x)dx \\
			& = E_f\left[\log\left(\frac{f(x)}{g(x)}\right)\right]
		\end{aligned}
		$$
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.6 Miscellanea in Model Selection}
		The AIC (Akaike Information Criterion, Akaike 1973) minimizes $K(\boldsymbol{\theta})$, however, \\
		\vspace{0.5em}
		it contains unknown parameter $f(x) \equiv f(x ; \boldsymbol{\theta}_0)$, so that we need to estimate it. \\
		\vspace{0.5em}
		As an estimator for $K(\boldsymbol{\theta})$, we consider
		$$
		K_n(\boldsymbol{\theta}) = -\frac{2}{n} \sum_{i=1}^{n} \log f(X_i ; \boldsymbol{\theta})
		$$
		Let $\overset{.}{K}_n(\boldsymbol{\theta})$ = $\partial K_n(\boldsymbol{\theta}) / \partial \boldsymbol{\theta}$, $\overset{..}{K}_n(\boldsymbol{\theta})$ = $\partial^2K_n(\boldsymbol{\theta}) / \partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'$, and let $\hat{\boldsymbol{\theta}}$ be the MLE of $\boldsymbol{\theta}$, then $\overset{.}{K}_n(\hat{\boldsymbol{\theta}})$ = 0, and let
		$$
		K_n(\hat{\boldsymbol{\theta}}) = - \frac{2}{n} \sum_{i=1}^{n} \log f(X_i ; \hat{\boldsymbol{\theta}}))
		$$
		Finally, the AIC is defined as the sum of $K_n(\hat{\boldsymbol{\theta}})$ and $\frac{2p}{n}$, i.e.,
		$$
		AIC = - \frac{2}{n} \sum_{i=1}^{n}\log f(X_i, \hat{\boldsymbol{\theta}}) + \frac{2p}{n}
		$$
		which is often used as a model selection criterion.
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.6 Miscellanea in Model Selection}
		\textbf{5.6.3 Justification of AIC} \\
		\vspace{0.5em}
		Here, we show the AIC is a good estimator for $K(\boldsymbol{\theta})$ by the following ; \\
		\vspace{2em}
		\textbf{Theorem 5.2} 
		\\
		$$ \begin{aligned}
			& K_n(\hat{\boldsymbol{\theta}}) = K(\hat{\boldsymbol{\theta}}) + R_n + o_p(n^{-1}), \\
			& where \;\; E(R_n) = -\frac{2p}{n} + o(n^{-1})
		\end{aligned}
		$$
		\vspace{2em}
		(Proof) \\
		By noting that $\overset{.}{K}_n(\hat{\boldsymbol{\theta}})$ = 0, and by the Taylor expansion, we have
		$$ \begin{aligned}
			K_n(\boldsymbol{\theta}_0) & = K_n(\hat{\boldsymbol{\theta}}) + (\boldsymbol{\theta}_0 - \hat{\boldsymbol{\theta}})'\overset{.}{K}_n(\hat{\boldsymbol{\theta}}) + \frac{1}{2} (\boldsymbol{\theta}_0 - \hat{\boldsymbol{\theta}})'\overset{..}{K}_n(\hat{\boldsymbol{\theta}})(\boldsymbol{\theta}_0 - \hat{\boldsymbol{\theta}}) + o_p(n^{-1}) \\
			& = K_n(\hat{\boldsymbol{\theta}}) + \frac{1}{2}(\boldsymbol{\theta}_0 - \hat{\boldsymbol{\theta}})'\overset{..}{K}_n(\hat{\boldsymbol{\theta}})(\boldsymbol{\theta}_0 - \hat{\boldsymbol{\theta}}) + o_p(n^{-1})
		\end{aligned}
		$$
		
		
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.6 Miscellanea in Model Selection}
		Also, by the Taylor expansion of $\overset{..}{K}_n(\hat{\boldsymbol{\theta}})$,
		$$
		\overset{..}{K}_n(\hat{\boldsymbol{\theta}}) = \overset{..}{K}_n(\boldsymbol{\theta}_0) + o_p(1) = 2 I(\boldsymbol{\theta}_0) + o_p(1),
		$$
		\\
		where
		\\
		$$
		I(\boldsymbol{\theta}_0) = - E_{\boldsymbol{\theta}_0} [\partial^2 \log f(X_1 ; \boldsymbol{\theta}) / \partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'] : Fisher \; information.
		$$
		\\
		Therefore,
		\\
		$$
		K_n(\hat{\boldsymbol{\theta}}) = K_n(\boldsymbol{\theta}_0) - (\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)'I_n(\boldsymbol{\theta}_0)(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0) + o_p(n^{-1})
		$$
		\\
		and further,
		$$ \begin{aligned}
			K(\hat{\boldsymbol{\theta}}) = K(\boldsymbol{\theta}_0) & + (\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)' \int \left[-2 \frac{\partial \log f(x;\boldsymbol{\theta})}{\partial \boldsymbol{\theta} }\right]_{\boldsymbol{\theta} = \boldsymbol{\theta}_0} f(x ; \boldsymbol{\theta}_0) dx \\
			& + (\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)' \left\{\int \left[- \frac{\partial^2 \log f(x; \boldsymbol{\theta})}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}'}\right]_{\boldsymbol{\theta} = \boldsymbol{\theta}_0} f(x;\boldsymbol{\theta}_0) dx\right\} (\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0) + o_p(n^{-1})
		\end{aligned}
		$$
		
		
		
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.6 Miscellanea in Model Selection}
		Hence,
		$$ \begin{aligned}
			K_n(\hat{\boldsymbol{\theta}}) & =  K(\hat{\boldsymbol{\theta}}) + \left\{K_n(\boldsymbol{\theta}_0) - K(\boldsymbol{\theta}_0) - 2(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)'I_n(\boldsymbol{\theta}_0)(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)\right\} + o_p(n^{-1}) \\
			& \equiv K(\hat{\boldsymbol{\theta}}) + R_n + o_p(n^{-1})
		\end{aligned}
		$$
		where $R_n$ = $K_n(\boldsymbol{\theta}_0) - K(\boldsymbol{\theta}_0) - 2(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)'I_n(\boldsymbol{\theta}_0)(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)$. 
		\\
		\vspace{0.5em}
		Now, by the asymptotic normality of the MLE,
		$$
		\sqrt{n}(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0) \rightarrow N_p(\textbf{0}, I^{-1}(\boldsymbol{\theta}_0))
		$$
		\\
		we have
		$$
		n(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)'I(\boldsymbol{\theta}_0)(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0) \rightarrow \chi^2_p
		$$
		\\
		and
		$$
		E_{\boldsymbol{\theta}_0} [n(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)'I(\boldsymbol{\theta}_0)(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}_0)] \rightarrow p
		$$
		\\
		\vspace{0.5em}
		Finally, by the WLLN(Weak Law of Large Numbers), we have
		$$
		K_n(\boldsymbol{\theta}_0) \overset{p}{\rightarrow} K(\boldsymbol{\theta}_0)
		$$
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.6 Miscellanea in Model Selection}
		
		\textbf{5.6.4 Application of AIC to model selection} \\
		\vspace{0.5em}
		Consider a multiple linear regression model
		$$
		\textbf{y} = \textbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}, \quad \boldsymbol{\varepsilon} \sim N(\textbf{0}, \sigma^2\textbf{I}),
		$$
		where $\sigma^2$ is unknown. Now,
		$$ \begin{aligned}
			K_n(\hat{\boldsymbol{\theta}}) & = -\frac{2}{n} \sum \log f(X_i ; \hat{\boldsymbol{\theta}}) \\
			& = - \frac{2}{n} \left\{- \frac{1}{2\sigma^2} \sum (Y_i - \textbf{x}_i'\hat{\boldsymbol{\beta}})^2\right\} + const \\
			& = \frac{SSE_p}{n\sigma^2} + const
		\end{aligned}
		$$
		
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\frame{ 
		\frametitle{5.6 Miscellanea in Model Selection}
		so that
		\\
		$$
		AIC = \frac{SSE_p}{n\sigma^2} + \frac{2p}{n} = \frac{1}{n}C_p + const
		$$
		\\
		Therefore, AIC gives the same result as the $C_p$. Also, BIC (Bayesian Information \\
		\vspace{0.5em}
		Criterion) is defined as 
		\\
		$$
		BIC = \frac{SSE_p}{n\sigma^2} + \frac{p \; \log(n)}{n}
		$$
		\\
		Note that BIC gives more penalty to model complexity than AIC, it tends to select \\
		\vspace{0.5em}
		simpler model than AIC.
	}
	
\end{document}